{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPsoHC+9WCLNmjX9/P1Qjg7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RM-RAMASAMY/decision_trees/blob/main/gbm_classifier_techniques/Adaboost_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ensemble Machine Learning Approach\n",
        "An ensemble is a composite model, combines a series of low performing classifiers with the aim of creating an improved classifier. Here, individual classifier vote and final prediction label returned that performs majority voting. Ensembles offer more accuracy than individual or base classifier.\n",
        "\n",
        "<br>\n",
        "\n",
        "Ensemble methods can parallelize by allocating each base learner to different-different machines. Finally, you can say Ensemble learning methods are meta-algorithms that combine several machine learning methods into a single predictive model to increase performance. Ensemble methods can decrease variance using bagging approach, bias using a boosting approach, or improve predictions using stacking approach.\n",
        "\n",
        "<br>\n",
        "\n",
        "![alt](https://images.datacamp.com/image/upload/f_auto,q_auto:best/v1542651255/image_1_joyt3x.png)\n",
        "\n",
        "#Bagging\n",
        "Bagging stands for bootstrap aggregation. It combines multiple learners in a way to reduce the variance of estimates. For example, random forest trains M Decision Tree, you can train M different trees on different random subsets of the data and perform voting for final prediction. Bagging ensembles methods are Random Forest and Extra Trees.\n",
        "\n",
        "#Boosting\n",
        "Boosting algorithms are a set of the low accurate classifier to create a highly accurate classifier. Low accuracy classifier (or weak classifier) offers the accuracy better than the flipping of a coin. Highly accurate classifier( or strong classifier) offer error rate close to 0. Boosting algorithm can track the model who failed the accurate prediction. Boosting algorithms are less affected by the overfitting problem. The following three algorithms have gained massive popularity in data science competitions.\n",
        "\n",
        "* AdaBoost (Adaptive Boosting)\n",
        "* Gradient Tree Boosting\n",
        "* XGBoost\n",
        "\n",
        "#Stacking\n",
        "Stacking(or stacked generalization) is an ensemble learning technique that combines multiple base classification models predictions into a new data set. This new data are treated as the input data for another classifier. This classifier employed to solve this problem. Stacking is often referred to as blending.\n",
        "\n",
        "![alt](https://images.datacamp.com/image/upload/f_auto,q_auto:best/v1542651255/image_2_pu8tu6.png)\n",
        "\n",
        "<br>\n",
        "\n",
        "On the basis of the arrangement of base learners, ensemble methods can be divided into two groups:\n",
        "* In parallel ensemble methods, base learners are generated in parallel for example. Random Forest.\n",
        "* In sequential ensemble methods, base learners are generated sequentially for example AdaBoost.\n",
        "\n",
        "<br>\n",
        "\n",
        "On the basis of the type of base learners, ensemble methods can be divided into two groups:\n",
        "* homogenous ensemble method uses the same type of base learner in each iteration.\n",
        "* heterogeneous ensemble method uses the different type of base learner in each iteration.\n",
        "\n",
        "<br>\n",
        "\n",
        "# AdaBoost Classifier\n",
        "Ada-boost or Adaptive Boosting is one of ensemble boosting classifier proposed by Yoav Freund and Robert Schapire in 1996. It combines multiple classifiers to increase the accuracy of classifiers. AdaBoost is an iterative ensemble method. AdaBoost classifier builds a strong classifier by combining multiple poorly performing classifiers so that you will get high accuracy strong classifier. The basic concept behind Adaboost is to set the weights of classifiers and training the data sample in each iteration such that it ensures the accurate predictions of unusual observations. Any machine learning algorithm can be used as base classifier if it accepts weights on the training set. Adaboost should meet two conditions:\n",
        "\n",
        "1. The classifier should be trained interactively on various weighed training examples.\n",
        "2. In each iteration, it tries to provide an excellent fit for these examples by minimizing training error.\n",
        "\n",
        "# How does the AdaBoost algorithm work?\n",
        "It works in the following steps:\n",
        "\n",
        "* Initially, Adaboost selects a training subset randomly.\n",
        "* It iteratively trains the AdaBoost machine learning model by selecting the training set based on the accurate prediction of the last training.\n",
        "* It assigns the higher weight to wrong classified observations so that in the next iteration these observations will get the high probability for classification.\n",
        "* Also, It assigns the weight to the trained classifier in each iteration according to the accuracy of the classifier. The more accurate classifier will get high weight.\n",
        "* This process iterate until the complete training data fits without any error or until reached to the specified maximum number of estimators.\n",
        "* To classify, perform a \"vote\" across all of the learning algorithms you built.\n",
        "\n",
        "<br>\n",
        "\n",
        "![](https://images.datacamp.com/image/upload/f_auto,q_auto:best/v1542651255/image_3_nwa5zf.png)"
      ],
      "metadata": {
        "id": "6288D_MByf6L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building Model in Python\n",
        "## Importing Required Libraries\n",
        "Let's first load the required libraries."
      ],
      "metadata": {
        "id": "j2gHKOPi2ZEL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load libraries\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn import datasets\n",
        "# Import train_test_split function\n",
        "from sklearn.model_selection import train_test_split\n",
        "#Import scikit-learn metrics module for accuracy calculation\n",
        "from sklearn import metrics\n"
      ],
      "metadata": {
        "id": "iLBGObOUrFKo"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Dataset\n",
        "In the model the building part, you can use the IRIS dataset, which is a very famous multi-class classification problem. This dataset comprises 4 features (sepal length, sepal width, petal length, petal width) and a target (the type of flower). This data has three types of flower classes: Setosa, Versicolour, and Virginica. The dataset is available in the scikit-learn library, or you can also download it from the UCI Machine Learning Library."
      ],
      "metadata": {
        "id": "j9jlFO-42M4M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n"
      ],
      "metadata": {
        "id": "YAfH2HtP1Nl0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split dataset\n",
        "To understand model performance, dividing the dataset into a training set and a test set is a good strategy.\n",
        "\n",
        "Let's split dataset by using function train_test_split(). you need to pass 3 parameters features, target, and test_set size."
      ],
      "metadata": {
        "id": "mobYWY3e2IdN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split dataset into training set and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) # 70% training and 30% test\n"
      ],
      "metadata": {
        "id": "nzf8UXlX1Nj7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building the AdaBoost Model\n",
        "Let's create the AdaBoost Model using Scikit-learn. AdaBoost uses Decision Tree Classifier as default Classifier."
      ],
      "metadata": {
        "id": "5ERG1PMu2Dek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create adaboost classifer object\n",
        "abc = AdaBoostClassifier(n_estimators=50,\n",
        "                         learning_rate=1)\n",
        "# Train Adaboost Classifer\n",
        "model = abc.fit(X_train, y_train)\n",
        "\n",
        "#Predict the response for test dataset\n",
        "y_pred = model.predict(X_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfX-XnvC1Nhr",
        "outputId": "6f193495-42a0-410e-c29a-125d9ff57c75"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> The most important parameters are base_estimator, n_estimators, and learning_rate. (Adaboost Classifier, Chris Albon)\n",
        "\n",
        "\n",
        "\n",
        "* base_estimator: It is a weak learner used to train the model. It uses DecisionTreeClassifier as default weak learner for training purpose. You can also specify different machine learning algorithms.\n",
        "* n_estimators: Number of weak learners to train iteratively.\n",
        "* learning_rate: It contributes to the weights of weak learners. It uses 1 as a default value.\n",
        "Evaluate Model\n",
        "\n",
        "Let's estimate, how accurately the classifier or model can predict the type of cultivars. Accuracy can be computed by comparing actual test set values and predicted values."
      ],
      "metadata": {
        "id": "c8FXRKb61sq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Accuracy, how often is the classifier correct?\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EyjcksT41NfT",
        "outputId": "150b5e37-4c0a-4566-90f5-aba02a67cfa6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9111111111111111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Well, you got an accuracy of 88.88%, considered as good accuracy.\n",
        "\n",
        "For further evaluation, you can also create a model using different Base Estimators.\n",
        "\n",
        "# Using Different Base Learners\n",
        "I have used SVC as a base estimator. You can use any ML learner as base estimator if it accepts sample weight such as Decision Tree, Support Vector Classifier."
      ],
      "metadata": {
        "id": "GWFuOVbU1gRT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load libraries\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "# Import Support Vector Classifier\n",
        "from sklearn.svm import SVC\n",
        "#Import scikit-learn metrics module for accuracy calculation\n",
        "from sklearn import metrics\n",
        "svc=SVC(probability=True, kernel='linear')\n",
        "\n",
        "# Create adaboost classifer object\n",
        "abc = AdaBoostClassifier(n_estimators=50, estimator=svc,learning_rate=1)\n",
        "\n",
        "# Train Adaboost Classifer\n",
        "model = abc.fit(X_train, y_train)\n",
        "\n",
        "#Predict the response for test dataset\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "\n",
        "# Model Accuracy, how often is the classifier correct?\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9Qc3a3g1Nc7",
        "outputId": "4cd27903-7779-42f9-9691-45ac60f31692"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9333333333333333\n"
          ]
        }
      ]
    }
  ]
}