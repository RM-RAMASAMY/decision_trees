{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOK12BsxxBuXh8HLhOGNLC4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RM-RAMASAMY/decision_trees/blob/main/GBM_Regressor_%26_Ranking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "!pip install catboost"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIbbM0-UUgyy",
        "outputId": "24559556-cb4b-4d97-d506-9832e5e8330c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: catboost in /usr/local/lib/python3.10/dist-packages (1.2.7)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.26.4)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.13.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.55.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.2.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (9.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import xgboost as xgb\n",
        "from catboost import CatBoostRegressor\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Generate sample data (replace with your actual data)\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 5)  # 100 samples, 5 features\n",
        "y = 2*X[:, 0] + 3*X[:, 1] - X[:, 2] + np.random.randn(100) # Example target variable\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "XajwON8IUU-T"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient Boosting Regressor Algorithms\n",
        "\n",
        "Gradient Boosting Regressor algorithms are machine learning techniques that predict continuous values by minimizing a regression loss function. They build an ensemble of decision trees sequentially to reduce errors and improve predictive accuracy. These models are commonly used in finance, healthcare, and other domains requiring accurate numeric predictions.\n",
        "\n",
        "Below are explanations of **XGBoost**, **LightGBM**, and **CatBoost** regression algorithms:\n",
        "\n",
        "---\n",
        "\n",
        "## **1. XGBoost (Extreme Gradient Boosting)**  \n",
        "XGBoost is a high-performance gradient boosting library designed for speed and efficiency. It provides robust support for regression tasks by optimizing various loss functions.\n",
        "\n",
        "### Features:\n",
        "- **Supports Regression Tasks**: Includes objectives like `reg:squarederror` (mean squared error), `reg:logistic` (logistic regression), and `reg:pseudohubererror` (Huber loss).\n",
        "- **Regularization**: L1 and L2 regularization are available for better generalization.\n",
        "- **Parallelization**: Optimized for multi-core processing.\n",
        "- **Custom Loss Functions**: Allows users to define custom regression loss functions.\n",
        "- **Handling Missing Data**: Efficiently manages missing values.\n",
        "\n",
        "### Common Use Case:\n",
        "Predicting housing prices or stock market trends using numerical features.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. LightGBM (Light Gradient Boosting Machine)**  \n",
        "LightGBM is a fast and efficient gradient boosting library designed for large-scale regression tasks with high-dimensional data.\n",
        "\n",
        "### Features:\n",
        "- **Objective Functions for Regression**:\n",
        "  - `regression`: For mean squared error (MSE).\n",
        "  - `l2`: For L2 loss (MSE) optimization.\n",
        "  - `huber`: For robust regression using Huber loss.\n",
        "- **Histogram-based Learning**: Splits data into bins, improving training speed and memory usage.\n",
        "- **Leaf-wise Growth**: Splits the leaf with the maximum loss reduction, leading to deeper trees and better performance.\n",
        "- **Support for Large Datasets**: Efficiently handles datasets with millions of rows and features.\n",
        "\n",
        "### Advantages in Regression:\n",
        "- Faster training for large-scale datasets.\n",
        "- Handles numerical data effectively with robust results.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. CatBoost (Categorical Boosting)**  \n",
        "CatBoost is a gradient boosting library tailored for datasets with categorical features and offers robust support for regression tasks.\n",
        "\n",
        "### Features:\n",
        "- **Native Support for Categorical Data**: Processes categorical features natively without manual preprocessing (like one-hot encoding).\n",
        "- **Regression Modes**:\n",
        "  - `RMSE`: Optimizes for root mean squared error.\n",
        "  - `MAE`: Optimizes for mean absolute error.\n",
        "  - `Quantile`: For quantile regression.\n",
        "- **Ordered Boosting**: Reduces overfitting by selecting training samples more carefully.\n",
        "- **GPU Acceleration**: Supports GPU training for faster computation.\n",
        "\n",
        "### Benefits in Regression:\n",
        "- Excels in datasets with categorical features.\n",
        "- Reduces overfitting compared to other gradient boosting frameworks.\n",
        "\n",
        "---\n",
        "\n",
        "## **Comparison of XGBoost, LightGBM, and CatBoost for Regression**:\n",
        "\n",
        "| Feature                | XGBoost                | LightGBM                  | CatBoost               |\n",
        "|------------------------|------------------------|---------------------------|------------------------|\n",
        "| **Speed**              | Moderate              | Fast                      | Moderate              |\n",
        "| **Ease of Use**        | Moderate              | Moderate                  | High                  |\n",
        "| **Categorical Features** | Requires preprocessing | Limited native support     | Fully supported       |\n",
        "| **Regression Objectives** | MSE, Huber, Custom    | MSE, Huber, Quantile       | RMSE, MAE, Quantile   |\n",
        "| **Scalability**        | High                  | Very High                 | Moderate              |\n",
        "\n",
        "---\n",
        "\n",
        "## **Use Case Scenarios**:\n",
        "\n",
        "- **XGBoost**: If you need a robust, general-purpose regression model with extensive community support.\n",
        "- **LightGBM**: If speed and scalability for large datasets are critical, especially for regression tasks.\n",
        "- **CatBoost**: If your data contains many categorical features and overfitting is a concern.\n",
        "\n",
        "Each algorithm has its strengths, and the choice often depends on your specific dataset and computational requirements.\n"
      ],
      "metadata": {
        "id": "JwcE91XaX11S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Regression techniques XGBoost, Catboost, LightGBM\n",
        "\n",
        "# 1. XGBoost\n",
        "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "xgb_pred = xgb_model.predict(X_test)\n",
        "xgb_rmse = np.sqrt(mean_squared_error(y_test, xgb_pred))\n",
        "print(f\"XGBoost RMSE: {xgb_rmse}\")\n",
        "\n",
        "# 2. CatBoost\n",
        "cat_model = CatBoostRegressor(iterations=100, random_seed=42, verbose=0) #verbose=0 to suppress output\n",
        "cat_model.fit(X_train, y_train)\n",
        "cat_pred = cat_model.predict(X_test)\n",
        "cat_rmse = np.sqrt(mean_squared_error(y_test, cat_pred))\n",
        "print(f\"CatBoost RMSE: {cat_rmse}\")\n",
        "\n",
        "\n",
        "# 3. LightGBM\n",
        "lgb_model = lgb.LGBMRegressor(random_state=42,force_col_wise=True, verbose=-1)\n",
        "lgb_model.fit(X_train, y_train)\n",
        "lgb_pred = lgb_model.predict(X_test)\n",
        "lgb_rmse = np.sqrt(mean_squared_error(y_test, lgb_pred))\n",
        "print(f\"LightGBM RMSE: {lgb_rmse}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C64GmqeIX0Up",
        "outputId": "ca7fbfcc-1af9-45d7-e98d-231f086f880b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost RMSE: 1.2797247448286968\n",
            "CatBoost RMSE: 1.116058972571808\n",
            "LightGBM RMSE: 1.0285364458917656\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient Boost Ranking Algorithms\n",
        "\n",
        "Gradient Boosting Ranking algorithms are machine learning techniques that rank items by optimizing a specific ranking loss function. They build an ensemble of decision trees in a sequential manner to minimize errors and improve the model's predictive performance for ranking tasks. These models are widely used in search engines, recommendation systems, and other ranking-based applications.\n",
        "\n",
        "Below are explanations of **XGBoost**, **LightGBM**, and **CatBoost** ranking algorithms:\n",
        "\n",
        "---\n",
        "\n",
        "## **1. XGBoost (Extreme Gradient Boosting)**  \n",
        "XGBoost is a high-performance gradient boosting library designed for speed and efficiency. It provides a specialized **ranking mode** that optimizes learning-to-rank objectives like **pairwise rank loss** or **LambdaRank**.\n",
        "\n",
        "### Features:\n",
        "- **Supports Ranking Tasks**: XGBoost can be configured for ranking by setting the objective to `rank:pairwise` or `rank:ndcg` (Normalized Discounted Cumulative Gain).\n",
        "- **Regularization**: XGBoost includes L1 and L2 regularization for better generalization.\n",
        "- **Parallelization**: Highly optimized for multi-core processing.\n",
        "- **Custom Loss Functions**: Allows users to define custom ranking loss functions.\n",
        "- **Handling Missing Data**: Efficiently manages missing values.\n",
        "\n",
        "### Common Use Case:\n",
        "Search engines use XGBoost to rank documents based on relevance scores.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. LightGBM (Light Gradient Boosting Machine)**  \n",
        "LightGBM is a fast and efficient gradient boosting library optimized for large-scale data and high-dimensional features. It provides support for ranking through its **learning-to-rank (LTR)** objectives.\n",
        "\n",
        "### Features:\n",
        "- **Objective Functions for Ranking**:\n",
        "  - `lambdarank`: Implements LambdaRank, a technique that directly optimizes metrics like NDCG.\n",
        "  - `rank_xendcg`: An enhanced version of LambdaRank for NDCG optimization.\n",
        "- **Histogram-based Learning**: Splits data into bins, improving training speed and memory usage.\n",
        "- **Leaf-wise Growth**: Splits the leaf with the maximum loss reduction, leading to deeper trees and better performance.\n",
        "- **Support for Large Datasets**: Handles datasets with millions of rows and features efficiently.\n",
        "\n",
        "### Advantages in Ranking:\n",
        "- Faster training for large-scale datasets.\n",
        "- Focused on optimizing ranking metrics like NDCG or Mean Average Precision (MAP).\n",
        "\n",
        "---\n",
        "\n",
        "## **3. CatBoost (Categorical Boosting)**  \n",
        "CatBoost is a gradient boosting library tailored for datasets with categorical features. It also supports ranking tasks with specialized ranking loss functions.\n",
        "\n",
        "### Features:\n",
        "- **Native Support for Categorical Data**: Efficiently processes categorical features without the need for manual preprocessing (like one-hot encoding).\n",
        "- **Ranking Modes**:\n",
        "  - `YetiRank`: Optimizes ranking metrics using pairwise comparisons.\n",
        "  - `QueryCrossEntropy`: For cross-entropy loss in ranking contexts.\n",
        "- **Ordered Boosting**: Reduces overfitting by selecting training samples more carefully.\n",
        "- **GPU Acceleration**: Supports GPU training for faster computation.\n",
        "\n",
        "### Benefits in Ranking:\n",
        "- Excels in datasets with categorical features.\n",
        "- Reduces overfitting compared to other gradient boosting frameworks.\n",
        "\n",
        "---\n",
        "\n",
        "## **Comparison of XGBoost, LightGBM, and CatBoost for Ranking**:\n",
        "\n",
        "| Feature                | XGBoost                | LightGBM                  | CatBoost               |\n",
        "|------------------------|------------------------|---------------------------|------------------------|\n",
        "| **Speed**              | Moderate              | Fast                      | Moderate              |\n",
        "| **Ease of Use**        | Moderate              | Moderate                  | High                  |\n",
        "| **Categorical Features** | Requires preprocessing | Limited native support     | Fully supported       |\n",
        "| **Ranking Objectives** | Pairwise, NDCG         | LambdaRank, XENDCG         | YetiRank, QueryCrossEntropy |\n",
        "| **Scalability**        | High                  | Very High                 | Moderate              |\n",
        "\n",
        "---\n",
        "\n",
        "## **Use Case Scenarios**:\n",
        "\n",
        "- **XGBoost**: If you need a robust, general-purpose ranking model with extensive community support.\n",
        "- **LightGBM**: If speed and scalability for large datasets are critical, especially for ranking tasks.\n",
        "- **CatBoost**: If your data contains many categorical features and overfitting is a concern.\n",
        "\n",
        "Each algorithm has its strengths, and the choice often depends on your specific dataset and computational requirements.\n"
      ],
      "metadata": {
        "id": "LzN-cQw4XRvT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Ranking techniques XGBoost, Catboost, LightGBM\n",
        "# Create a query data for ranking\n",
        "qids = np.random.randint(0, 10, size=100) # Example query ids (10 queries)\n",
        "\n",
        "# Combine features, target, and query ids into a DataFrame\n",
        "train_data = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(5)])\n",
        "train_data['target'] = y\n",
        "train_data['qid'] = qids\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test, qid_train, qid_test = train_test_split(\n",
        "    train_data.drop(['target','qid'], axis=1),\n",
        "    train_data['target'],\n",
        "    train_data['qid'],\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "# 1. XGBoost\n",
        "xgb_model = xgb.XGBRanker(objective='rank:pairwise', random_state=42)\n",
        "xgb_model.fit(X_train, y_train, group=qid_train.value_counts(sort=False).values)\n",
        "xgb_pred = xgb_model.predict(X_test)\n",
        "xgb_rmse = np.sqrt(mean_squared_error(y_test, xgb_pred))\n",
        "print(f\"XGBoost RMSE: {xgb_rmse}\")\n",
        "\n",
        "\n",
        "# 2. CatBoost\n",
        "cat_model = CatBoostRegressor(iterations=100, random_seed=42, verbose=0, loss_function='RMSE')\n",
        "cat_model.fit(X_train, y_train)  # No specific ranking loss in CatBoost, using RMSE\n",
        "cat_pred = cat_model.predict(X_test)\n",
        "cat_rmse = np.sqrt(mean_squared_error(y_test, cat_pred))\n",
        "print(f\"CatBoost RMSE: {cat_rmse}\")\n",
        "\n",
        "\n",
        "# 3. LightGBM\n",
        "# 3. LightGBM\n",
        "# Convert y_train to integer labels for ranking\n",
        "# Here, we rank based on the order of the original target values within each group\n",
        "y_train_ranked = y_train.groupby(qid_train).rank(method='first').astype(int)\n",
        "\n",
        "lgb_model = lgb.LGBMRanker(random_state=42, force_col_wise=True, verbose=-1, objective='lambdarank')\n",
        "lgb_model.fit(X_train, y_train_ranked, group=qid_train.value_counts(sort=False).values) # Use ranked labels\n",
        "lgb_pred = lgb_model.predict(X_test)\n",
        "lgb_rmse = np.sqrt(mean_squared_error(y_test, lgb_pred)) # You might need a different evaluation metric for ranking\n",
        "print(f\"LightGBM RMSE: {lgb_rmse}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZEliwQSWNlS",
        "outputId": "ba6f9840-900c-495b-802e-4d71cd2b364f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost RMSE: 3.7391786532001214\n",
            "CatBoost RMSE: 1.116058972571808\n",
            "LightGBM RMSE: 3.5333778669703766\n"
          ]
        }
      ]
    }
  ]
}